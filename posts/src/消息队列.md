---
layout: page
title: 消息队列
categories: [kafka]
tags: [kafka]
keywords: 
description: kafka
mathjax: true

---

## kafka

### 安装

下载安装包：http://kafka.apache.org/quickstart

准备java8环境：cat /opt/sun-java8/sun-java8.sh >> ~/.bashrc && source ~/.bashrc

wget https://downloads.apache.org/kafka/3.0.0/kafka_2.13-3.0.0.tgz

tar -xzvf kafka_2.13-3.0.0.tgz && cd kafka_2.13-3.0.0

启动Zookeeper：

nohup bin/zookeeper-server-start.sh config/zookeeper.properties &> zookeeper-server-start.log &  # Start the ZooKeeper service

``` shell
dataDir=/home/work/.tmp/zookeeper
clientPort=2181
maxClientCnxns=0
admin.enableServer=false
```

启动Kafka：

nohup bin/kafka-server-start.sh config/server.properties &> kafka-server-start.log &   # Start the Kafka broker service

```bash
broker.id=0  
log.dirs=/home/work/.tmp/kafka-logs    
num.partitions=1

# 生效时间
# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.                                                                                                                                                                                                         
# The number of messages to accept before forcing a flush of data to disk
log.flush.interval.messages=1000
# The maximum amount of time a message can sit in a log before we force a flush
log.flush.interval.ms=1000

# 时效时间
# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.
# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168  # 7 days
# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.                                                                                                                       
log.retention.bytes=10737418240  # 10GB
# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824  # 1GB
# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000

zookeeper.connect=localhost:2181
zookeeper.connection.timeout.ms=18000

```

### 使用

创建一个*topic*：

bin/kafka-topics.sh --create --topic  *my-topic*  --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

#### python

用pykafka.KafkaClient消费，用kafka.KafkaProducer生产。



#### shell

发送消息： bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test  // 执行kafka-console-producer.sh命令后，在控制台输入文本，每行是一条消息。结束输入通过Ctrl-C退出。

接收数据：bin/kafka-console-consumer.sh --topic my-topic --from-beginning --bootstrap-server localhost:9092

查看所有Topic： bin/kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --list

kafka-topics.sh使用，以下是主要参数项：

```bash
--alter 修改副本的数量
--bootstrap-server 连接的kafka地址，不再需要直接连接zookeeper地址
--command-config 传递给管理客户端的配置文件
--config 主题配置覆盖
--create 创建新的topic
--delete 删除topic
--delete-config 删除已经存在的topic配置
--describe 描述topic详情
--list 列出所有topic
--partitions 分区数量
--replica-assignment 指定副本对应节点关系
--replication-factor 主题的副本数
--topic 主题名
```

